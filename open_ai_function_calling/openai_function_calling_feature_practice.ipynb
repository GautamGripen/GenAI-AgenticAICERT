{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce915b8c",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling Tutorial\n",
    "Learn how to extend ChatGPT's capabilities with custom functions\n",
    "\n",
    "## What is Function Calling?\n",
    "- Allows ChatGPT to call external functions/APIs\n",
    "- Converts natural language requests into structured function calls\n",
    "- Enables ChatGPT to interact with real-world systems\n",
    "\n",
    "## What we'll cover:\n",
    "1. Basic function calling concepts\n",
    "2. Function definition and schema\n",
    "3. Simple examples (calculator, weather)\n",
    "4. Advanced examples (database queries, API calls)\n",
    "5. Best practices and error handling\n",
    "6. Real-world applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f86c1313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\hp\\miniconda3\\lib\\site-packages (1.93.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81c403aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ca2d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Check if the API key above works fine\n",
    "\n",
    "client = openai.OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e32255c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Response:\n",
      "(a+b)^2 = a^2 + 2ab + b^2\n"
     ]
    }
   ],
   "source": [
    "#Now lets call the OpenAI API \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's (a+b)^2)?\"}       #user input\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Traditional Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3317b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try creating a function in which if we type in the prompt, it should give answer.\n",
    "def get_open_ai_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are specilized in GEN AI so answer the question like a GEN AI specilist\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}   # this will be the input by user\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    print(\"Traditional Response:\")\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31867643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Response:\n",
      "As of the latest data, the highest-paid sportsperson in the world is typically a professional boxer or soccer player. The exact player can vary year by year based on endorsements, salaries, and prize earnings. Some of the highest-paid athletes in recent years have included Lionel Messi, Cristiano Ronaldo, Conor McGregor, and Floyd Mayweather Jr.\n"
     ]
    }
   ],
   "source": [
    "get_open_ai_response(\"who is the highest paid sportsperson in the world?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d0b2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can always get the code for prompts from the Open AI website> Select the model> playground - https://platform.openai.com/playground/prompts?models=gpt-4o \n",
    "# you can then click on view code after getting the answer.\n",
    "# using model gpt 4.0\n",
    "response = client.responses.create(\n",
    "  model=\"gpt-4o\",\n",
    "  input=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"input_text\",\n",
    "          \"text\": \"What are your thoughts on Boeing as a company considering their current plane crashes\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"msg_686ea037a570819d8397fce2228c34c9059afff4a9c68ad0\",\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"output_text\",\n",
    "          \"text\": \"Boeing has faced significant challenges due to high-profile plane crashes, particularly the two Boeing 737 MAX accidents in 2018 and 2019. These tragedies led to the global grounding of the MAX fleet and raised serious questions about safety practices and regulatory oversight. Boeing's response included updates to the plane’s software and improvements in its safety culture.\\n\\nThe company's reputation took a hit, impacting financial performance and trust within the aviation industry. However, Boeing has been working to address these issues by enhancing transparency, reevaluating safety protocols, and collaborating more closely with regulatory bodies like the FAA.\\n\\nThe long-term impact on Boeing will largely depend on its ability to restore confidence among airlines and passengers, continue to innovate, and demonstrate a commitment to safety and quality.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  text={\n",
    "    \"format\": {\n",
    "      \"type\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  reasoning={},\n",
    "  tools=[],\n",
    "  temperature=1,\n",
    "  max_output_tokens=2048,\n",
    "  top_p=1,\n",
    "  store=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d8cf1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try creating a function in which if we type in the prompt, it should give answer.\n",
    "def get_open_ai_response2(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are specialised in Formula 1 racing and you are technically sound in Aerodynamics, automobile engineering and vehicle design\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    print(\"Traditional Response:\")\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1154bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Response:\n",
      "In Formula 1 racing, Pirelli provides two types of wet weather tires to handle rainy conditions:\n",
      "\n",
      "1. **Intermediate Tires (Green-marked):** These tires are designed for use in light rain or on a drying track where there are patches of water. They have a tread pattern that can disperse water to prevent aquaplaning, while still offering a reasonable amount of grip on a drying track. Intermediate tires are typically used when the track is too wet for slicks but not wet enough to need full wet tires.\n",
      "\n",
      "2. **Wet Tires (Blue-marked):** Also known as full wets, these tires are designed for heavy rain conditions. They have a deeper and more aggressive tread pattern compared to intermediate tires, which allows them to disperse a larger volume of water. This reduces the risk of aquaplaning in heavy rain and provides maximum grip in significant wet conditions.\n",
      "\n",
      "The choice between intermediates and wets depends on the intensity of the rain, the amount of standing water on the track, and evolving weather conditions during a race or session. Teams and drivers must make strategic decisions about which tire to use based on these factors.\n"
     ]
    }
   ],
   "source": [
    "get_open_ai_response2(\"What tyre will I use for rains in Formula 1 racing ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7586b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Response:\n",
      "Drag is a force that opposes an object's motion through a fluid, which can be either a liquid or a gas. In the context of Formula 1 racing, drag refers specifically to the resistance an F1 car faces as it moves through the air. The aerodynamic efficiency of a car is a critical factor in its performance, as excessive drag can significantly reduce top speed and fuel efficiency.\n",
      "\n",
      "Drag in F1 comes from several sources, primarily the shape of the car and the speed at which it is traveling. The components contributing to drag include:\n",
      "\n",
      "1. **Form Drag**: Generated by the shape of the car and the airflow separation that occurs around it. The smoother and more streamlined the design, the less form drag is produced.\n",
      "\n",
      "2. **Skin Friction Drag**: Caused by the friction of air against the surface of the car. It is a function of the car's surface area and the roughness of the skin.\n",
      "\n",
      "3. **Induced Drag**: Linked to the lift generated by the wings and is a byproduct of downforce production. As the car creates downforce to improve grip, it also induces additional drag.\n",
      "\n",
      "4. **Interference Drag**: Occurs where different airstreams meet and interact around complex aerodynamic components.\n",
      "\n",
      "F1 teams strive to minimize drag while maximizing downforce, which is essential for maintaining high speeds and handling effectively around corners. This balance is achieved through advanced design techniques, computational fluid dynamics (CFD), and wind tunnel testing, all aimed at optimizing the car's aerodynamic profile.\n"
     ]
    }
   ],
   "source": [
    "get_open_ai_response2(\"What is drag?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f1adc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Response:\n",
      "Slipstreaming, also known as drafting, is a tactic used in racing, including Formula 1, to exploit the aerodynamic effects produced by a leading vehicle. When a car moves through the air, it creates a zone of lower air pressure and reduced air resistance behind it. This zone, called a slipstream or wake, allows a following vehicle to experience decreased aerodynamic drag.\n",
      "\n",
      "In practical terms, when a race car follows another car closely, it encounters less air resistance due to the disturbed air behind the leading car. As a result, the following car requires less engine power to maintain or increase speed. Drivers can use slipstreaming to conserve energy or fuel, making it strategically beneficial, especially on long straights.\n",
      "\n",
      "Slipstreaming is also a critical component during overtaking maneuvers. By reducing aerodynamic drag, a following car can achieve a higher speed than it might otherwise be able to, providing an opportunity to move alongside and overtake the lead car as they exit the wake.\n",
      "\n",
      "However, staying too close in turbulent air can also affect downforce and the handling of the trailing car due to disrupted airflow over its aerodynamic surfaces. This balance between gaining speed and maintaining control is a key aspect of race strategy.\n"
     ]
    }
   ],
   "source": [
    "get_open_ai_response2(\"What is slipstream?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8297b",
   "metadata": {},
   "source": [
    "NOW One thing we should know that if we give prompt in \"User\" section instead of \"System\", it will still return the answer. But then we will have to mention it again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b12627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 * 23 = 345\n"
     ]
    }
   ],
   "source": [
    "# Define a simple calculator function\n",
    "def calculate(operation: str, a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Perform basic mathematical operations\n",
    "    \n",
    "    Args:\n",
    "        operation: The operation to perform (+, -, *, /)\n",
    "        a: First number\n",
    "        b: Second number\n",
    "    \n",
    "    Returns:\n",
    "        Result of the calculation\n",
    "    \"\"\"                                               #This is Docstring eclosed in \"\"\" \"\"\" so that functiont will be known what function does... \n",
    "    if operation == \"+\":\n",
    "        return a + b\n",
    "    elif operation == \"-\":\n",
    "        return a - b\n",
    "    elif operation == \"*\":\n",
    "        return a * b\n",
    "    elif operation == \"/\":\n",
    "        return a / b if b != 0 else \"Error: Division by zero\"\n",
    "    else:\n",
    "        return \"Error: Unsupported operation\"\n",
    "\n",
    "# Test the function\n",
    "result = calculate(\"*\", 15, 23)\n",
    "print(f\"15 * 23 = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b748c5c",
   "metadata": {},
   "source": [
    "# Now we will try to call the above function by Open AI(LLM) where in user will give input in natural language *(eg: what would be the product of 2 and 3/ what is 3 devided by 2...etc)\n",
    "# Once the input is given by the user in Natural language, LLM will decide which function to call..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00202b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Function schema defined!\n",
      "{\n",
      "  \"name\": \"calculate\",\n",
      "  \"description\": \"Perform basic mathematical operations like addition, subtraction, multiplication, and division\",\n",
      "  \"parameters\": {\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "      \"operation\": {\n",
      "        \"type\": \"string\",\n",
      "        \"description\": \"The mathematical operation to perform\",\n",
      "        \"enum\": [\n",
      "          \"+\",\n",
      "          \"-\",\n",
      "          \"*\",\n",
      "          \"/\"\n",
      "        ]\n",
      "      },\n",
      "      \"a\": {\n",
      "        \"type\": \"number\",\n",
      "        \"description\": \"The first number\"\n",
      "      },\n",
      "      \"b\": {\n",
      "        \"type\": \"number\",\n",
      "        \"description\": \"The second number\"\n",
      "      }\n",
      "    },\n",
      "    \"required\": [\n",
      "      \"operation\",\n",
      "      \"a\",\n",
      "      \"b\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Defining Function Schema\n",
    "\n",
    "import json\n",
    "# Define the function schema for OpenAI\n",
    "calculator_function = {\n",
    "    \"name\": \"calculate\",\n",
    "    \"description\": \"Perform basic mathematical operations like addition, subtraction, multiplication, and division\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The mathematical operation to perform\",\n",
    "                \"enum\": [\"+\", \"-\", \"*\", \"/\"]\n",
    "            },\n",
    "            \"a\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The first number\"\n",
    "            },\n",
    "            \"b\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The second number\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"operation\", \"a\", \"b\"]                       #These are required parameters.\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📋 Function schema defined!\")\n",
    "print(json.dumps(calculator_function, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f7810",
   "metadata": {},
   "source": [
    "#### Basic Function Calling Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_calculator(user_message: str):\n",
    "    \"\"\"\n",
    "    Chat with OpenAI using function calling for calculations\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Yo are calculation specilist.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Make the initial request with function definition\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        functions=[calculator_function],             #We are asking to use this function\n",
    "        function_call=\"auto\"  # Let OpenAI decide when to call functions\n",
    "    )\n",
    "    \n",
    "    message = response.choices[0].message\n",
    "    if message.function_call:\n",
    "        function_name = message.function_call.name\n",
    "        function_args = json.loads(message.function_call.arguments)\n",
    "        \n",
    "        print(f\"🔧 OpenAI wants to call: {function_name}\")\n",
    "        print(f\"📝 Arguments: {function_args}\")\n",
    "        if function_name == \"calculate\":\n",
    "            result = calculate(**function_args)\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": None,\n",
    "                \"function_call\": {\n",
    "                    \"name\": function_name,\n",
    "                    \"arguments\": json.dumps(function_args)\n",
    "                }\n",
    "            })\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": str(result)\n",
    "            })\n",
    "            \n",
    "            # Get final response from OpenAI\n",
    "            final_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages\n",
    "            )\n",
    "        return final_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4546f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Function Calling Result:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test the function calling\n",
    "result = chat_with_calculator(\"who is the PM of India\")   # we should not get any result as we have integrated the calculator function\n",
    "print(\"🎯 Function Calling Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94e333a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 OpenAI wants to call: calculate\n",
      "📝 Arguments: {'operation': '*', 'a': 15, 'b': 23}\n",
      "🎯 Function Calling Result:\n",
      "Step 1: 15 * 23 = 345\n",
      "\n",
      "Now add 45 to the result:\n",
      "\n",
      "Step 2: 345 + 45 = 390\n",
      "\n",
      "Therefore, 15 * 23 + 45 = 390.\n"
     ]
    }
   ],
   "source": [
    "# Test the function calling\n",
    "result = chat_with_calculator(\"What's 15 * 23 + 45? Please calculate step by step.\")\n",
    "print(\"🎯 Function Calling Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce4294",
   "metadata": {},
   "source": [
    "Lets Try with Another function now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1689c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city: str, country: str = \"US\") -> str:\n",
    "    \"\"\"\n",
    "    Get current weather for a city\n",
    "    \n",
    "    Args:\n",
    "        city: Name of the city\n",
    "        country: Country code (default: US)\n",
    "    \n",
    "    Returns:\n",
    "        Weather information as string\n",
    "    \"\"\"\n",
    "    # Using a mock weather API for demonstration\n",
    "    # In real implementation, you'd use actual weather API\n",
    "    mock_weather_data = {\n",
    "        \"new york\": {\"temp\": 22, \"condition\": \"sunny\", \"humidity\": 60},\n",
    "        \"london\": {\"temp\": 15, \"condition\": \"cloudy\", \"humidity\": 80},\n",
    "        \"tokyo\": {\"temp\": 28, \"condition\": \"rainy\", \"humidity\": 75}\n",
    "    }\n",
    "    \n",
    "    city_key = city.lower()\n",
    "    if city_key in mock_weather_data:\n",
    "        data = mock_weather_data[city_key]\n",
    "        return f\"Weather in {city}: {data['temp']}°C, {data['condition']}, humidity: {data['humidity']}%\"\n",
    "    else:\n",
    "        return f\"Weather data n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c09e0fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Function schema defined!\n",
      "{\n",
      "  \"name\": \"get_weather\",\n",
      "  \"description\": \"Find out the weather forcast of the city\",\n",
      "  \"parameters\": {\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "      \"city\": {\n",
      "        \"type\": \"string\",\n",
      "        \"description\": \"The city to get the weather forcast\"\n",
      "      },\n",
      "      \"country\": {\n",
      "        \"type\": \"string\",\n",
      "        \"description\": \"The weather of the country\"\n",
      "      }\n",
      "    },\n",
      "    \"required\": [\n",
      "      \"city\",\n",
      "      \"country\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Define the function schema for OpenAI\n",
    "weather_function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Find out the weather forcast of the city\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city to get the weather forcast\",\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The weather of the country\"\n",
    "            }\n",
    "            \n",
    "        },\n",
    "        \"required\": [\"city\", \"country\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📋 Function schema defined!\")\n",
    "print(json.dumps(weather_function, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f718b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_weather(user_message: str):\n",
    "    \"\"\"\n",
    "    Chat with OpenAI using function calling for calculations\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You can predict weather like an expert\"},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Make the initial request with function definition\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        functions=[weather_function],   #We are asking to use this function\n",
    "        function_call=\"auto\"  # Let OpenAI decide when to call functions\n",
    "    )\n",
    "    \n",
    "    message = response.choices[0].message\n",
    "    if message.function_call:\n",
    "        function_name = message.function_call.name\n",
    "        function_args = json.loads(message.function_call.arguments)\n",
    "        \n",
    "        print(f\"🔧 OpenAI wants to call: {function_name}\")\n",
    "        print(f\"📝 Arguments: {function_args}\")\n",
    "        if function_name == \"get_weather\":\n",
    "            result = get_weather(**function_args)\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": None,\n",
    "                \"function_call\": {\n",
    "                    \"name\": function_name,\n",
    "                    \"arguments\": json.dumps(function_args)\n",
    "                }\n",
    "            })\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": str(result)\n",
    "            })\n",
    "            \n",
    "            # Get final response from OpenAI\n",
    "            final_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages\n",
    "            )\n",
    "        return final_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "686ba9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 OpenAI wants to call: get_weather\n",
      "📝 Arguments: {'city': 'Tokyo', 'country': 'Japan'}\n",
      "🎯 Function Calling Result:\n",
      "The weather in Tokyo is currently 28°C with rain and 75% humidity.\n"
     ]
    }
   ],
   "source": [
    "# Test the function calling\n",
    "result = chat_with_weather(\"What's the weather at Tokyo\")\n",
    "print(\"🎯 Function Calling Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1237ae7",
   "metadata": {},
   "source": [
    "# Create a handler that can work with multiple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2848532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionHandler:\n",
    "    def __init__(self):\n",
    "        self.functions = {\n",
    "            \"calculate\": calculate,\n",
    "            \"get_weather\": get_weather\n",
    "        }\n",
    "        \n",
    "        self.function_schemas = [\n",
    "            calculator_function,\n",
    "            weather_function\n",
    "        ]\n",
    "    \n",
    "    def call_function(self, function_name: str, arguments: dict):\n",
    "        \"\"\"Call the appropriate function with given arguments\"\"\"\n",
    "        if function_name in self.functions:\n",
    "            return self.functions[function_name](**arguments)\n",
    "        else:\n",
    "            return f\"Function {function_name} not found\"\n",
    "    \n",
    "    def chat(self, user_message: str):\n",
    "        \"\"\"Enhanced chat with multiple function support\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            functions=self.function_schemas,\n",
    "            function_call=\"auto\"\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message\n",
    "        \n",
    "        if message.function_call:\n",
    "            function_name = message.function_call.name\n",
    "            function_args = json.loads(message.function_call.arguments)\n",
    "            \n",
    "            print(f\"🔧 Calling function: {function_name}\")\n",
    "            print(f\"📝 Arguments: {function_args}\")\n",
    "            \n",
    "            # Execute the function\n",
    "            result = self.call_function(function_name, function_args)\n",
    "            \n",
    "            # Continue conversation with function result\n",
    "            messages.extend([\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": None,\n",
    "                    \"function_call\": {\n",
    "                        \"name\": function_name,\n",
    "                        \"arguments\": json.dumps(function_args)\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"function\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": str(result)\n",
    "                }\n",
    "            ])\n",
    "            \n",
    "            final_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            return final_response.choices[0].message.content\n",
    "        \n",
    "        return message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e04d6618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Multi-function handler ready!\n"
     ]
    }
   ],
   "source": [
    "handler = FunctionHandler()\n",
    "print(\"🤖 Multi-function handler ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f56cb0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Calling function: get_weather\n",
      "📝 Arguments: {'city': 'Tokyo', 'country': 'Japan'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The temperature in Tokyo is currently 28°C with rainy weather and a humidity of 75%.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler.chat(\"what is the Temperature Tokyo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf079eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ff392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328275b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41964c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9d6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de582258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
