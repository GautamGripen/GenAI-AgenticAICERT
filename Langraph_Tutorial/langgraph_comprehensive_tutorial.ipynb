{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7852ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from typing import Annotated, Dict, List, Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Setup your API KEYs\"\"\"\n",
    "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        openai_key=getpass.getpass(\"Enter your OPENAI API KEY\")\n",
    "        os.environ[\"OPENAI_API_KEY\"]=openai_key\n",
    "\n",
    "    if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
    "        langsmith_key=getpass.getpass(\"Enter your langsmith api key\")\n",
    "        if langsmith_key:\n",
    "            os.environ[\"LANGSMITH_API_KEY\"]=langsmith_key\n",
    "            os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "            os.environ[\"LANGCHAIN_PROJECT\"] = \"Langgraph tutorial\"\n",
    "        else:\n",
    "            print(\"skipping langsmith setup\")\n",
    "\n",
    "    print(\"Environment Setup is Complete\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e4d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Setup is Complete\n"
     ]
    }
   ],
   "source": [
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ee0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Step -1 - Defining State\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages              #This is a method             \n",
    "from langchain_core.messages import HumanMessage, AIMessage   # these are classes. one with camel casing name are classes\n",
    "\n",
    "class State(TypedDict):\n",
    "    # messages will store our conversation history\n",
    "    # add_messages is a special function that append new message instead of replacing them\n",
    "\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01aace8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o\",\n",
    "    temperature = 0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9ca129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-3  - lets create a node (which is just a Like a function)\n",
    "def chatbot_node(state: State)->Dict[str, Any]:\n",
    "    messages = state['messages']\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7539e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step - 4 - Setup Edges in Graph\n",
    "\n",
    "#Now for this we will invoke the class \"StateGraph with an object\"\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
    "#lets add edges\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "simple_chatbot = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a88761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now tpo genrate the message for graph, we can do below\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Hello, how are you\")]\n",
    "}\n",
    "result = simple_chatbot.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5bcd328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, how are you\n",
      "AI: Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "#now for graph, we should always put the message in the loop for iteration\n",
    "for i, message in enumerate(result['messages']):\n",
    "    if isinstance(message, HumanMessage):\n",
    "        print(f\"Human: {message.content}\")\n",
    "\n",
    "    if isinstance(message, AIMessage):\n",
    "        print(f\"AI: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99193870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the above part let us just store it in a function and just re-use it\n",
    "def test_simple(user_input):\n",
    "    initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Hello, how are you\")]\n",
    "    }\n",
    "    result = simple_chatbot.invoke(initial_state)\n",
    "\n",
    "    for i, message in enumerate(result['messages']):\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"Human: {message.content}\")\n",
    "\n",
    "        if isinstance(message, AIMessage):\n",
    "            print(f\"AI: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca8505f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph-checkpoint-sqlite in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (2.0.10)\n",
      "Requirement already satisfied: aiosqlite>=0.20 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langgraph-checkpoint-sqlite) (0.21.0)\n",
      "Requirement already satisfied: langgraph-checkpoint>=2.0.21 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langgraph-checkpoint-sqlite) (2.1.0)\n",
      "Requirement already satisfied: sqlite-vec>=0.1.6 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langgraph-checkpoint-sqlite) (0.1.6)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from aiosqlite>=0.20->langgraph-checkpoint-sqlite) (4.13.2)\n",
      "Requirement already satisfied: langchain-core>=0.2.38 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.3.66)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.10.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.4.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.11.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\miniconda3\\envs\\mydevenvironment\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "#Now lets us again add memory to the chatbot\n",
    "#lets use sql_lite database\n",
    "!pip install langgraph-checkpoint-sqlite\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6ea794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "checkpointer=MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3619854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder_with_memory = StateGraph(State)\n",
    "\n",
    "graph_builder_with_memory.add_node(\"chatbot\", chatbot_node)\n",
    "graph_builder_with_memory.add_edge(START, \"chatbot\")\n",
    "graph_builder_with_memory.add_edge(\"chatbot\", END)\n",
    "\n",
    "chatbot_with_memory = graph_builder_with_memory.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64dc499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple(user_input):\n",
    "    config={\"configurable\":{\"thread_id\":\"conversation_1\"}}   #Threads per conversation\n",
    "    initial_state={\n",
    "\n",
    "        \"messages\":[HumanMessage(content=user_input)]\n",
    "    }\n",
    "    result=chatbot_with_memory.invoke(initial_state,config)\n",
    "    for i, message in enumerate(result['messages']):\n",
    "        if isinstance(message,HumanMessage):\n",
    "            print(f\"Human:{message.content}\")\n",
    "\n",
    "        if isinstance(message,AIMessage):\n",
    "            print(f\"AI:{message.content}\")\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ada217a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:which is the faster bike in the world\n",
      "AI:As of the latest information available, the Dodge Tomahawk is often cited as one of the fastest motorcycles in the world. It features a V10 engine from a Dodge Viper and is claimed to have a top speed of around 350 mph (560 km/h). However, it should be noted that the Tomahawk is more of a concept vehicle and not street-legal.\n",
      "\n",
      "For production motorcycles, the Kawasaki Ninja H2R is frequently mentioned as the fastest. It's a track-only bike with a supercharged 998cc inline-four engine, capable of reaching speeds over 240 mph (386 km/h).\n",
      "\n",
      "Keep in mind that these figures are subject to change as new models are developed and performance enhancements are made in the motorcycle industry.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='which is the faster bike in the world', additional_kwargs={}, response_metadata={}, id='33f9ea67-a547-471a-8154-84f003bba61f'),\n",
       "  AIMessage(content=\"As of the latest information available, the Dodge Tomahawk is often cited as one of the fastest motorcycles in the world. It features a V10 engine from a Dodge Viper and is claimed to have a top speed of around 350 mph (560 km/h). However, it should be noted that the Tomahawk is more of a concept vehicle and not street-legal.\\n\\nFor production motorcycles, the Kawasaki Ninja H2R is frequently mentioned as the fastest. It's a track-only bike with a supercharged 998cc inline-four engine, capable of reaching speeds over 240 mph (386 km/h).\\n\\nKeep in mind that these figures are subject to change as new models are developed and performance enhancements are made in the motorcycle industry.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 148, 'prompt_tokens': 15, 'total_tokens': 163, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BnihrNO0FGDCXz5Sv6eDq46GreoIm', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f97a47dd-e462-4e83-8351-9e0e7c37ced3-0', usage_metadata={'input_tokens': 15, 'output_tokens': 148, 'total_tokens': 163, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt=input(\"Enter your prompt:\")\n",
    "test_simple(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64ea616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ Testing Calculator Tool:\n",
      "The result of 2 + 3 * 4 is 14\n",
      "The result of sqrt(16) + 5 is 9.0\n",
      "\n",
      "‚è∞ Testing Time Tool:\n",
      "Current time is: 2025-06-29 15:03:43\n"
     ]
    }
   ],
   "source": [
    "# Now lets make it as an API\n",
    "#Tool integration\n",
    "#so that the agent can interact with outside world.\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import math\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate a mathematical expression safely.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression to evaluate (e.g., \"2 + 3 * 4\")\n",
    "        \n",
    "    Returns:\n",
    "        The result of the calculation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safe evaluation of mathematical expressions\n",
    "        # Only allow basic math operations\n",
    "        allowed_names = {\n",
    "            k: v for k, v in math.__dict__.items() if not k.startswith(\"__\")\n",
    "        }\n",
    "        allowed_names.update({\"abs\": abs, \"round\": round})\n",
    "        \n",
    "        result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating {expression}: {str(e)}\"\n",
    "\n",
    "@tool  \n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current time.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"Current time is: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Test our tools\n",
    "print(\"üßÆ Testing Calculator Tool:\")\n",
    "print(calculator.invoke({\"expression\": \"2 + 3 * 4\"}))\n",
    "print(calculator.invoke({\"expression\": \"sqrt(16) + 5\"}))\n",
    "\n",
    "print(\"\\n‚è∞ Testing Time Tool:\")\n",
    "print(get_current_time.invoke({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db51eac",
   "metadata": {},
   "source": [
    "#Create a Tool-Enabled Chatbot\n",
    "\n",
    "Now we need to:\n",
    "1. Bind tools to our LLM\n",
    "2. Create a tool-calling node\n",
    "3. Add conditional logic to decide when to use tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc8e0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tool-enabled chatbot components created!\n",
      "- LLM knows about our tools\n",
      "- Chatbot node can generate tool calls\n",
      "- Tool node can execute the tools\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "# Step 1: Create LLM with tools\n",
    "tools = [calculator, get_current_time]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Step 2: Create nodes\n",
    "def chatbot_with_tools(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Chatbot that can use tools\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Step 3: Create tool node using LangGraph's prebuilt ToolNode\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "print(\"‚úÖ Tool-enabled chatbot components created!\")\n",
    "print(\"- LLM knows about our tools\")\n",
    "print(\"- Chatbot node can generate tool calls\") \n",
    "print(\"- Tool node can execute the tools\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9132fb0",
   "metadata": {},
   "source": [
    "Add Conditional Logic\n",
    "\n",
    "We need to route between:\n",
    "- **Tools**: If the LLM wants to use a tool\n",
    "- **End**: If the LLM gives a final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dabd06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conditional logic created!\n",
      "This function decides whether to:\n",
      "- Use tools (if LLM made tool calls)\n",
      "- End conversation (if LLM gave final response)\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def should_continue(state: State) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Determine if we should use tools or end the conversation.\n",
    "    \n",
    "    Returns:\n",
    "        \"tools\" if the last message has tool calls\n",
    "        \"__end__\" if we should end\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]                    #we are using -1 so that we can fetch the last message(previous) from the user. This is how it works.\n",
    "    \n",
    "    # If the last message has tool calls, we should use tools\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:               #we are using hasattr which is Hash String as we had used dictionary type before\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"__end__\"\n",
    "\n",
    "print(\"‚úÖ Conditional logic created!\")\n",
    "print(\"This function decides whether to:\")\n",
    "print(\"- Use tools (if LLM made tool calls)\")\n",
    "print(\"- End conversation (if LLM gave final response)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7a0a4",
   "metadata": {},
   "source": [
    "Build the Tool-Enabled Graph\n",
    "\n",
    "Now let's put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f859eadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tool-enabled chatbot with memory created!\n",
      "Flow: START ‚Üí chatbot ‚Üí [tools OR end] ‚Üí (if tools) ‚Üí chatbot ‚Üí ...\n"
     ]
    }
   ],
   "source": [
    "#creating the graph \n",
    "tools_graph_builder = StateGraph(State)\n",
    "\n",
    "#adding nodes\n",
    "tools_graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
    "tools_graph_builder.add_node(\"tools\", tool_node )\n",
    "\n",
    "#add edges\n",
    "tools_graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "#add conditional edges\n",
    "tools_graph_builder.add_conditional_edges(\n",
    "    \"chatbot\", #from the chatbot node\n",
    "    should_continue,  # use this function to decide what logic to use or end\n",
    "    {\n",
    "        \"tools\":\"tools\", # If should_continue returns \"tools\", go to tools node\n",
    "        \"_end\":END       # If should_continue returns \"__end__\", end the graph\n",
    "    }\n",
    ")\n",
    "\n",
    "# after tools, go back to chatbot\n",
    "tools_graph_builder.add_edge(\"tools\",END)\n",
    "\n",
    "#Compile with memory\n",
    "tool_chatbot = tools_graph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "print(\"‚úÖ Tool-enabled chatbot with memory created!\")\n",
    "print(\"Flow: START ‚Üí chatbot ‚Üí [tools OR end] ‚Üí (if tools) ‚Üí chatbot ‚Üí ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8a05d",
   "metadata": {},
   "source": [
    "### Test the Tool-Enabled Chatbot\n",
    "\n",
    "Let's test our enhanced chatbot with some math problems and time queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0788f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Testing Tool-Enabled Chatbot\n",
      "==================================================\n",
      "üìù Test 1: Math Calculation\n",
      "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
      "ü§ñ AI: [Calling tool: calculator]\n",
      "üîß Tool: The result of 15 * 7 + sqrt(144) is 117.0\n",
      "\n",
      "üìù Test 2: Current Time\n",
      "üë§ Human: What time is it right now?\n",
      "ü§ñ AI: [Calling tool: get_current_time]\n",
      "üîß Tool: Current time is: 2025-06-29 15:03:46\n",
      "\n",
      "‚úÖ Tool integration working perfectly!\n"
     ]
    }
   ],
   "source": [
    "def test_tool_chatbot():\n",
    "    print(\"üõ†Ô∏è Testing Tool-Enabled Chatbot\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"tool_test_1\"}}\n",
    "    \n",
    "    # Test 1: Math calculation\n",
    "    print(\"üìù Test 1: Math Calculation\")\n",
    "    result1 = tool_chatbot.invoke(\n",
    "        {\"messages\": [HumanMessage(content=\"What's 15 * 7 + sqrt(144)?\")]},\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    for message in result1[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"üë§ Human: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                print(f\"ü§ñ AI: [Calling tool: {message.tool_calls[0]['name']}]\")\n",
    "            else:\n",
    "                print(f\"ü§ñ AI: {message.content}\")\n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"üîß Tool: {message.content}\")\n",
    "    \n",
    "    print(\"\\nüìù Test 2: Current Time\")\n",
    "    result2 = tool_chatbot.invoke(\n",
    "        {\"messages\": [HumanMessage(content=\"What time is it right now?\")]},\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    # Show only new messages\n",
    "    new_messages = result2[\"messages\"][len(result1[\"messages\"]):]\n",
    "    for message in new_messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"üë§ Human: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                print(f\"ü§ñ AI: [Calling tool: {message.tool_calls[0]['name']}]\")\n",
    "            else:\n",
    "                print(f\"ü§ñ AI: {message.content}\")\n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"üîß Tool: {message.content}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Tool integration working perfectly!\")\n",
    "    return result2\n",
    "\n",
    "# Test the tool-enabled chatbot\n",
    "tool_result = test_tool_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d765e5",
   "metadata": {},
   "source": [
    "Part 5: Human-in-the-Loop Workflows {#part5}\n",
    "\n",
    "Sometimes AI agents need human oversight or approval before taking actions. LangGraph makes this easy with **interrupts**.\n",
    "\n",
    "### What are Interrupts?\n",
    "\n",
    "Interrupts pause the graph execution at specific nodes, allowing humans to:\n",
    "- Review what the agent plans to do\n",
    "- Modify the state if needed\n",
    "- Approve or reject actions\n",
    "- Provide additional guidance\n",
    "\n",
    "### Step 1: Create an Agent that Asks for Help\n",
    "\n",
    "Let's create an agent that can request human assistance when it's unsure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecf92244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced State with human assistance flag\n",
    "class HumanLoopState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    ask_human: bool  # Flag to request human help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33b66bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def request_human_help(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Request help from a human supervisor.\n",
    "    \n",
    "    Args:\n",
    "        question: The question or situation where human help is needed\n",
    "        \n",
    "    Returns:\n",
    "        Confirmation that help has been requested\n",
    "    \"\"\"\n",
    "    return f\"Human help requested for: {question}\"\n",
    "\n",
    "# Add the new tool to our toolkit\n",
    "human_tools = [calculator, get_current_time, request_human_help]\n",
    "# now we will bind\n",
    "llm_with_human_tools = llm.bind_tools(human_tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5a20134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Human-in-the-loop nodes created!\n",
      "- chatbot_with_human_help: Can request human assistance\n",
      "- human_node: Handles human intervention\n"
     ]
    }
   ],
   "source": [
    "#create a node with human assistant logic\n",
    "#Lets create a chatbot/ LLM node\n",
    "def chatbot_with_human_help(state: HumanLoopState) -> Dict[str, Any]:\n",
    "    messages - state[\"messages\"]\n",
    "    response = llm_with_human_tools.invoke(messages)\n",
    "    ask_human = False\n",
    "    if hasattr(response, \"tool_calls\"):\n",
    "        for tool_call in response.tool_calls:\n",
    "            if tool_call['name'] == 'request_human_help':\n",
    "                ask_human = True\n",
    "    return {\"messages\": [response], \"ask_human\": ask_human}\n",
    "\n",
    "#now lets create a human node\n",
    "def human_node(state: HumanLoopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Human intervention node - this is where humans can provide input\n",
    "    \"\"\"\n",
    "    # In a real application, this would wait for human input\n",
    "    # For this demo, we'll simulate human response\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        tool_call = last_message.tool_calls[0]\n",
    "        if tool_call['name'] == 'request_human_help':\n",
    "            # Simulate human response\n",
    "            human_response = \"I've reviewed your question. Please proceed with the calculation and provide a detailed explanation.\"\n",
    "            tool_message = ToolMessage(\n",
    "                content = human_response,\n",
    "                tool_call_id = tool_call['id']\n",
    "            )\n",
    "            return{\"message\": [tool_message],\n",
    "                   \"ask_human\": False}\n",
    "        \n",
    "    return {\"ask_human\": False}   \n",
    "\n",
    "\n",
    "print(\"‚úÖ Human-in-the-loop nodes created!\")\n",
    "print(\"- chatbot_with_human_help: Can request human assistance\")\n",
    "print(\"- human_node: Handles human intervention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1db11",
   "metadata": {},
   "source": [
    "### Step 3: Build the Human-in-the-Loop Graph\n",
    "### Now we will create edges and conditional edges accordinly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa782707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Human-in-the-loop chatbot created!\n",
      "Key feature: interrupt_before=['human'] pauses for human input\n"
     ]
    }
   ],
   "source": [
    "def route_human_loop(state: HumanLoopState) -> Literal[\"human\", \"tools\", \"__end__\"]:\n",
    "    \"\"\"Route based on whether human help is needed or tools should be called\"\"\"\n",
    "    \n",
    "    if state.get(\"ask_human\", False):\n",
    "        return \"human\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check for tool calls (excluding request_human_help)\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            if tool_call['name'] != 'request_human_help':\n",
    "                return \"tools\"\n",
    "            else:\n",
    "                return \"__end__\"\n",
    "\n",
    "# Build the graph\n",
    "human_loop_builder = StateGraph(HumanLoopState)\n",
    "\n",
    "# Add nodes\n",
    "human_loop_builder.add_node(\"chatbot\", chatbot_with_human_help)\n",
    "human_loop_builder.add_node(\"tools\", ToolNode(human_tools))\n",
    "human_loop_builder.add_node(\"human\", human_node)\n",
    "\n",
    "# Add edges\n",
    "human_loop_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# Add conditional routing/ edges\n",
    "human_loop_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_human_loop,\n",
    "    {\n",
    "        \"human\": \"human\",\n",
    "        \"tools\": \"tools\", \n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# After human or tools, go back to END (because if we got back to Chatbot, it will be going to infinite loop)\n",
    "human_loop_builder.add_edge(\"human\", END)\n",
    "human_loop_builder.add_edge(\"tools\", END)\n",
    "\n",
    "# Compile with interrupt before human node\n",
    "human_loop_chatbot = human_loop_builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[\"human\"]  # This pauses execution before human node\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Human-in-the-loop chatbot created!\")\n",
    "print(\"Key feature: interrupt_before=['human'] pauses for human input\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4a4a8",
   "metadata": {},
   "source": [
    "### Step 4: Test Human-in-the-Loop\n",
    "\n",
    "Let's test the interrupt functionality:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c50887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_human_loop():\n",
    "    print(\"ü§ù Testing Human-in-the-Loop\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"human_loop_test\"}}\n",
    "    \n",
    "    # Request that requires human help\n",
    "    print(\"üìù Asking agent to request human help:\")\n",
    "    \n",
    "    initial_result = human_loop_chatbot.invoke(\n",
    "        {\"messages\": [HumanMessage(content=\"I need help with a complex decision. Can you request human assistance?\")]},\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    for message in initial_result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"üë§ Human: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                print(f\"ü§ñ AI: [Requesting human help]\")\n",
    "            else:\n",
    "                print(f\"ü§ñ AI: {message.content}\")\n",
    "    \n",
    "    # Check if execution was interrupted\n",
    "    state = human_loop_chatbot.get_state(config)\n",
    "    print(f\"\\nüîç Current state:\")\n",
    "    print(f\"Next node to execute: {state.next}\")\n",
    "    print(f\"Ask human flag: {state.values.get('ask_human', False)}\")\n",
    "    \n",
    "    if state.next == (\"human\",):\n",
    "        print(\"\\n‚úÖ Execution interrupted! Human intervention required.\")\n",
    "        print(\"In a real app, a human would now review and provide input.\")\n",
    "        \n",
    "        # Continue execution (simulating human approval)\n",
    "        print(\"\\n‚ñ∂Ô∏è Continuing execution (simulating human input)...\")\n",
    "        final_result = human_loop_chatbot.invoke(None, config)\n",
    "        \n",
    "        # Show the final messages\n",
    "        new_messages = final_result[\"messages\"][len(initial_result[\"messages\"]):]\n",
    "        for message in new_messages:\n",
    "            if isinstance(message, ToolMessage):\n",
    "                print(f\"üë®‚Äçüíº Human: {message.content}\")\n",
    "            elif isinstance(message, AIMessage):\n",
    "                print(f\"ü§ñ AI: {message.content}\")\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Test the human-in-the-loop functionality\n",
    "human_loop_result = test_human_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyDevEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
